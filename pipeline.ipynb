{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 0.1 **Function: get_time_embedding** \n",
    "This function generates time embeddings for a given timestep using sinusoidal functions. These embeddings are often used in natural language processing (NLP) tasks or transformers for representing time or position information.\n",
    "\n",
    "### **Parameters:**\n",
    "- **`timesteps`**: The time step or position for which you want to generate an embedding.\n",
    "\n",
    "### **Steps:**\n",
    "1. **Calculate Frequencies:**\n",
    "   ```python\n",
    "   freqs = torch.pow(10000, -torch.arange(start=0, end=160, dtype=torch.float32) / 160)\n",
    "   ```\n",
    "   - **`torch.arange(start=0, end=160, dtype=torch.float32)`**: This generates a sequence of numbers from 0 to 159.\n",
    "   - **`torch.pow(10000, -... / 160)`**: This computes a scaling factor based on powers of 10,000, which is commonly used in sinusoidal embeddings.\n",
    "\n",
    "2. **Compute Scaled Time Step:**\n",
    "   ```python\n",
    "   x = torch.tensor([timesteps], dtype=torch.float32)[:, None] * freqs[None]\n",
    "   ```\n",
    "   - **`torch.tensor([timesteps], dtype=torch.float32)[:, None]`**: Converts the timestep into a tensor and reshapes it for broadcasting.\n",
    "   - **`* freqs[None]`**: Multiplies the timestep by the calculated frequencies, resulting in a tensor suitable for trigonometric operations.\n",
    "\n",
    "3. **Generate Embeddings:**\n",
    "   ```python\n",
    "   return torch.cat([torch.cos(x), torch.sin(x)], dim=-1)\n",
    "   ```\n",
    "   - **`torch.cos(x)` and `torch.sin(x)`**: Applies cosine and sine functions to the scaled timestep.\n",
    "   - **`torch.cat([...], dim=-1)`**: Concatenates the cosine and sine values along the last dimension, effectively doubling the size of the embedding.\n",
    "\n",
    "### **Output:**\n",
    "- The function returns a tensor of shape `(1, 320)` (since 160 cosine values and 160 sine values are concatenated), representing the time embedding for the given timestep.\n",
    "\n",
    "### **Use Case:**\n",
    "- **Sinusoidal Embeddings**: These embeddings are used in transformer models to encode positional information without relying on learned embeddings, allowing the model to generalize better to sequences of different lengths.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Embedding for timestep 5 :\n",
      "tensor([[ 2.8366e-01,  7.9154e-03, -2.5334e-01, -4.8417e-01, -6.7484e-01,\n",
      "         -8.2086e-01, -9.2179e-01, -9.8004e-01, -9.9991e-01, -9.8670e-01,\n",
      "         -9.4608e-01, -8.8366e-01, -8.0468e-01, -7.1384e-01, -6.1519e-01,\n",
      "         -5.1215e-01, -4.0752e-01, -3.0353e-01, -2.0187e-01, -1.0384e-01,\n",
      "         -1.0342e-02,  7.8026e-02,  1.6090e-01,  2.3812e-01,  3.0968e-01,\n",
      "          3.7566e-01,  4.3626e-01,  4.9171e-01,  5.4229e-01,  5.8831e-01,\n",
      "          6.3008e-01,  6.6791e-01,  7.0211e-01,  7.3297e-01,  7.6079e-01,\n",
      "          7.8583e-01,  8.0834e-01,  8.2857e-01,  8.4672e-01,  8.6300e-01,\n",
      "          8.7758e-01,  8.9065e-01,  9.0234e-01,  9.1280e-01,  9.2216e-01,\n",
      "          9.3053e-01,  9.3800e-01,  9.4468e-01,  9.5065e-01,  9.5598e-01,\n",
      "          9.6073e-01,  9.6498e-01,  9.6877e-01,  9.7215e-01,  9.7516e-01,\n",
      "          9.7785e-01,  9.8025e-01,  9.8240e-01,  9.8430e-01,  9.8601e-01,\n",
      "          9.8753e-01,  9.8888e-01,  9.9009e-01,  9.9116e-01,  9.9212e-01,\n",
      "          9.9298e-01,  9.9374e-01,  9.9442e-01,  9.9503e-01,  9.9557e-01,\n",
      "          9.9605e-01,  9.9648e-01,  9.9686e-01,  9.9720e-01,  9.9751e-01,\n",
      "          9.9778e-01,  9.9802e-01,  9.9823e-01,  9.9843e-01,  9.9860e-01,\n",
      "          9.9875e-01,  9.9889e-01,  9.9901e-01,  9.9912e-01,  9.9921e-01,\n",
      "          9.9930e-01,  9.9937e-01,  9.9944e-01,  9.9950e-01,  9.9956e-01,\n",
      "          9.9960e-01,  9.9965e-01,  9.9969e-01,  9.9972e-01,  9.9975e-01,\n",
      "          9.9978e-01,  9.9980e-01,  9.9982e-01,  9.9984e-01,  9.9986e-01,\n",
      "          9.9988e-01,  9.9989e-01,  9.9990e-01,  9.9991e-01,  9.9992e-01,\n",
      "          9.9993e-01,  9.9994e-01,  9.9994e-01,  9.9995e-01,  9.9996e-01,\n",
      "          9.9996e-01,  9.9996e-01,  9.9997e-01,  9.9997e-01,  9.9998e-01,\n",
      "          9.9998e-01,  9.9998e-01,  9.9998e-01,  9.9998e-01,  9.9999e-01,\n",
      "          9.9999e-01,  9.9999e-01,  9.9999e-01,  9.9999e-01,  9.9999e-01,\n",
      "          9.9999e-01,  9.9999e-01,  9.9999e-01,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00,\n",
      "         -9.5892e-01, -9.9997e-01, -9.6738e-01, -8.7497e-01, -7.3796e-01,\n",
      "         -5.7113e-01, -3.8770e-01, -1.9879e-01, -1.3194e-02,  1.6256e-01,\n",
      "          3.2394e-01,  4.6813e-01,  5.9371e-01,  7.0031e-01,  7.8838e-01,\n",
      "          8.5890e-01,  9.1320e-01,  9.5282e-01,  9.7941e-01,  9.9459e-01,\n",
      "          9.9995e-01,  9.9695e-01,  9.8697e-01,  9.7123e-01,  9.5084e-01,\n",
      "          9.2676e-01,  8.9982e-01,  8.7076e-01,  8.4019e-01,  8.0863e-01,\n",
      "          7.7653e-01,  7.4424e-01,  7.1207e-01,  6.8026e-01,  6.4900e-01,\n",
      "          6.1844e-01,  5.8871e-01,  5.5989e-01,  5.3204e-01,  5.0521e-01,\n",
      "          4.7943e-01,  4.5470e-01,  4.3102e-01,  4.0840e-01,  3.8680e-01,\n",
      "          3.6622e-01,  3.4663e-01,  3.2799e-01,  3.1027e-01,  2.9345e-01,\n",
      "          2.7748e-01,  2.6234e-01,  2.4798e-01,  2.3438e-01,  2.2149e-01,\n",
      "          2.0929e-01,  1.9774e-01,  1.8681e-01,  1.7648e-01,  1.6670e-01,\n",
      "          1.5746e-01,  1.4872e-01,  1.4045e-01,  1.3264e-01,  1.2526e-01,\n",
      "          1.1829e-01,  1.1170e-01,  1.0548e-01,  9.9598e-02,  9.4043e-02,\n",
      "          8.8797e-02,  8.3842e-02,  7.9162e-02,  7.4742e-02,  7.0568e-02,\n",
      "          6.6627e-02,  6.2905e-02,  5.9390e-02,  5.6071e-02,  5.2938e-02,\n",
      "          4.9979e-02,  4.7186e-02,  4.4548e-02,  4.2057e-02,  3.9706e-02,\n",
      "          3.7486e-02,  3.5390e-02,  3.3411e-02,  3.1543e-02,  2.9779e-02,\n",
      "          2.8113e-02,  2.6541e-02,  2.5057e-02,  2.3655e-02,  2.2332e-02,\n",
      "          2.1083e-02,  1.9904e-02,  1.8791e-02,  1.7740e-02,  1.6747e-02,\n",
      "          1.5811e-02,  1.4926e-02,  1.4091e-02,  1.3303e-02,  1.2559e-02,\n",
      "          1.1857e-02,  1.1193e-02,  1.0567e-02,  9.9761e-03,  9.4181e-03,\n",
      "          8.8913e-03,  8.3939e-03,  7.9244e-03,  7.4811e-03,  7.0626e-03,\n",
      "          6.6676e-03,  6.2946e-03,  5.9425e-03,  5.6101e-03,  5.2962e-03,\n",
      "          5.0000e-03,  4.7203e-03,  4.4562e-03,  4.2070e-03,  3.9716e-03,\n",
      "          3.7495e-03,  3.5397e-03,  3.3417e-03,  3.1548e-03,  2.9783e-03,\n",
      "          2.8117e-03,  2.6544e-03,  2.5059e-03,  2.3658e-03,  2.2334e-03,\n",
      "          2.1085e-03,  1.9905e-03,  1.8792e-03,  1.7741e-03,  1.6748e-03,\n",
      "          1.5811e-03,  1.4927e-03,  1.4092e-03,  1.3304e-03,  1.2559e-03,\n",
      "          1.1857e-03,  1.1194e-03,  1.0567e-03,  9.9763e-04,  9.4182e-04,\n",
      "          8.8914e-04,  8.3940e-04,  7.9245e-04,  7.4812e-04,  7.0627e-04,\n",
      "          6.6676e-04,  6.2946e-04,  5.9425e-04,  5.6101e-04,  5.2963e-04]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_time_embedding(timesteps):\n",
    "\n",
    "    # shape: (160,)  # Position\n",
    "    freqs = torch.pow(10000, -torch.arange(start=0, end=160, dtype=torch.float32) / 160)\n",
    "\n",
    "    # shape: (1, 160)\n",
    "    x = torch.tensor([timesteps], dtype=torch.float32)[:, None] * freqs[None]\n",
    "\n",
    "    # Shape: (1, 160 * 2)\n",
    "    return torch.cat([torch.cos(x), torch.sin(x)], dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "# Example \n",
    "timesteps = 5 \n",
    "embedding = get_time_embedding(timesteps)\n",
    "\n",
    "print(\"Time Embedding for timestep\", timesteps, \":\")\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's define the `generate` function line by line:\n",
    "\n",
    "```python\n",
    "WIDTH = 512\n",
    "HEIGHT = 512 \n",
    "LATENTS_WIDTH = WIDTH // 512 \n",
    "LATENTS_HEIGHT = HEIGHT // 512\n",
    "```\n",
    "- **Sets the dimensions of the generated image**: `WIDTH` and `HEIGHT` are set to 512 pixels. `LATENTS_WIDTH` and `LATENTS_HEIGHT` are calculated by dividing the dimensions by 512, defining the size of the latent variables.\n",
    "\n",
    "```python\n",
    "def generate(\n",
    "        prompt,\n",
    "        uncond_prompt=None,\n",
    "        input_image=None,\n",
    "        strength=0.8,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=7.5,\n",
    "        sampler_name=\"ddpm\",\n",
    "        n_inference_steps=50,\n",
    "        models={},\n",
    "        seed=None,\n",
    "        device=None,\n",
    "        idle_device=None,\n",
    "        tokenizer=None,\n",
    "):\n",
    "```\n",
    "- **Function Definition**: This defines the `generate` function and its parameters:\n",
    "  - `prompt`: The main text prompt.\n",
    "  - `uncond_prompt`: Optional unconditional text prompt.\n",
    "  - `input_image`: Optional input image.\n",
    "  - `strength`: Controls the influence of the input image.\n",
    "  - `do_cfg`: Boolean flag for classifier-free guidance.\n",
    "  - `cfg_scale`: Guidance scale for CFG.\n",
    "  - `sampler_name`: Name of the sampler (e.g., \"ddpm\").\n",
    "  - `n_inference_steps`: Number of inference steps.\n",
    "  - `models`: Dictionary of models (`clip`, `encoder`, `diffusion`, `decoder`).\n",
    "  - `seed`: Optional random seed.\n",
    "  - `device`: Device for computation (e.g., CPU or GPU).\n",
    "  - `idle_device`: Optional device to move models when not in use.\n",
    "  - `tokenizer`: Tokenizer for text processing.\n",
    "\n",
    "```python\n",
    "    with torch.no_grad():\n",
    "        if not 0 < strength <= 1:\n",
    "            raise ValueError(\"strength must be between 0 and 1\")\n",
    "```\n",
    "- **Disable Gradient Calculation**: `torch.no_grad()` is used to disable gradient calculations, which saves memory and computations since we don't need backpropagation.\n",
    "- **Validate Strength**: Checks if `strength` is between 0 and 1. If not, raises a `ValueError`.\n",
    "\n",
    "```python\n",
    "        if idle_device:\n",
    "            to_idle = lambda x: x.to(idle_device)\n",
    "        else:\n",
    "            to_idle = lambda x: x \n",
    "```\n",
    "- **Device Handling**: Defines a lambda function `to_idle` to move models to the `idle_device` if specified, or leave them on the current device.\n",
    "\n",
    "```python\n",
    "        generator = torch.Generator(device=device)\n",
    "        if seed is None:\n",
    "            generator.seed()\n",
    "        else:\n",
    "            generator.manual_seed(seed)\n",
    "```\n",
    "- **Random Number Generator**: Initializes a random number generator on the specified device. If `seed` is provided, uses it to set the generator's seed; otherwise, seeds it randomly.\n",
    "\n",
    "```python\n",
    "        clip = models[\"clip\"]\n",
    "        clip.to(device)\n",
    "```\n",
    "- **Load CLIP Model**: Moves the CLIP model to the specified device.\n",
    "\n",
    "```python\n",
    "        if do_cfg:\n",
    "            cond_tokens = tokenizer.batch_encode_plus(\n",
    "                [prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids\n",
    "            cond_tokens = torch.tensor(cond_tokens, dtype=torch.long, device=device)\n",
    "            cond_context = clip(cond_tokens)\n",
    "            uncond_tokens = tokenizer.batch_encode_plus(\n",
    "                [uncond_prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids\n",
    "            uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=device)\n",
    "            uncond_context = clip(uncond_tokens)\n",
    "            context = torch.cat([cond_context, uncond_context])\n",
    "```\n",
    "- **Conditional and Unconditional Prompts**: If `do_cfg` is `True`, tokenizes both the main prompt and the unconditional prompt. Then, it generates context embeddings using the CLIP model and concatenates them.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "        else:\n",
    "            tokens = tokenizer.batch_encode_plus(\n",
    "                [prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids\n",
    "            tokens = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "            context = clip(tokens)\n",
    "```\n",
    "- **Single Prompt Handling**: If `do_cfg` is `False`, only tokenizes the main prompt and generates the context embedding.\n",
    "\n",
    "```python\n",
    "        to_idle(clip)\n",
    "```\n",
    "- **Move CLIP to Idle Device**: Moves the CLIP model to the `idle_device` if specified.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "        if sampler_name == \"ddpm\":\n",
    "            sampler = DDPMSampler(generator)\n",
    "            sampler.set_inference_timesteps(n_inference_steps)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sampler value %s. \")\n",
    "```\n",
    "- **Sampler Initialization**: Initializes the sampler based on the `sampler_name`. If `ddpm`, sets up `DDPMSampler` and configures the number of inference steps.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "        latent_shape = (1, 4, LATENTS_HEIGHT, LATENTS_WIDTH)\n",
    "        if input_image:\n",
    "            encoder = models[\"encoder\"]\n",
    "            encoder.to(device)\n",
    "            input_image_tensor = input_image.resize((WIDTH, HEIGHT))\n",
    "            input_image_tensor = np.array(input_image_tensor)\n",
    "            input_image_tensor = torch.tensor(input_image_tensor, dtype=torch.float32, device=device)\n",
    "            input_image_tensor = rescale(input_image_tensor, (0, 255), (-1, 1))\n",
    "            input_image_tensor = input_image_tensor.unsqueeze(0)\n",
    "            input_image_tensor = input_image_tensor.permute(0, 3, 1, 2)\n",
    "            encoder_noise = torch.randn(latent_shape, generator=generator, device=device)\n",
    "            latents = encoder(input_image_tensor, encoder_noise)\n",
    "            sampler.set_strength(strength=strength)\n",
    "            latents = sampler.add_noise(latents, sampler.timesteps[0])\n",
    "            to_idle(encoder)\n",
    "```\n",
    "- **Input Image Handling**: If `input_image` is provided, processes and encodes it:\n",
    "  - Resizes the image.\n",
    "  - Converts it to a NumPy array and then to a PyTorch tensor.\n",
    "  - Rescales the tensor values.\n",
    "  - Adds a batch dimension and permutes the dimensions.\n",
    "  - Generates noise and encodes the image to latents.\n",
    "  - Adds noise to the latents based on the specified `strength`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "        else:\n",
    "            latents = torch.randn(latent_shape, generator=generator, device=device)\n",
    "```\n",
    "- **Random Latent Initialization**: If no input image is provided, initializes random latents.\n",
    "\n",
    "```python\n",
    "        diffusion = models['diffusion']\n",
    "        diffusion.to(device)\n",
    "        timesteps = tqdm(sampler.timesteps)\n",
    "```\n",
    "- **Load Diffusion Model**: Moves the diffusion model to the specified device and prepares the timesteps for the diffusion process.\n",
    "\n",
    "```python\n",
    "        for i, timesteps in enumerate(timesteps):\n",
    "            time_embedding = get_time_embedding(timesteps).to(device)\n",
    "            model_input = latents\n",
    "            if do_cfg:\n",
    "                model_input = model_input.repeat(2, 1, 1, 1)\n",
    "            model_output = diffusion(model_input, context, time_embedding)\n",
    "            if do_cfg:\n",
    "                output_cond, output_uncond = model_output.chunk(2)\n",
    "                model_output = cfg_scale * (output_cond - output_uncond) + output_uncond\n",
    "            latents = sampler.step(timesteps, latents, model_output)\n",
    "```\n",
    "- **Diffusion Process**: For each timestep:\n",
    "  - Generates a time embedding.\n",
    "  - Prepares the model input.\n",
    "  - If `do_cfg` is `True`, repeats the latents.\n",
    "  - Uses the diffusion model to predict noise.\n",
    "  - If `do_cfg` is `True`, combines the conditional and unconditional outputs.\n",
    "  - Updates the latents using the sampler.\n",
    "\n",
    "```python\n",
    "        to_idle(diffusion)\n",
    "        decoder = models[\"decoder\"]\n",
    "        decoder.to(device)\n",
    "        images = decoder(latents)\n",
    "        to_idle(decoder)\n",
    "```\n",
    "- **Decode Latents**: Moves the diffusion model to the `idle_device` if specified, loads the decoder model, and decodes the latents to generate the final image.\n",
    "\n",
    "```python\n",
    "        images = rescale(images, (-1, 1), (0, 255), clamp=True)\n",
    "        images = images.permute(0, 2, 3, 1)\n",
    "        images = images.to(\"cpu\", torch.uint8).numpy()\n",
    "        return images[0]\n",
    "```\n",
    "- **Post-process and Return**: Rescales and clamps the image, permutes the dimensions for correct image format, converts it to a NumPy array, and returns the generated image.\n",
    "\n",
    "This should provide a detailed understanding of each line of the `generate` function. Let me know if you have any further questions or need clarification on any specific part! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 512\n",
    "HEIGHT = 512 \n",
    "LATENTS_WIDTH = WIDTH // 512 \n",
    "LATENTS_HEIGHT = HEIGHT // 512 \n",
    "\n",
    "\n",
    "def generate(\n",
    "        prompt,\n",
    "        uncond_prompt=None,\n",
    "        input_image=None,\n",
    "        strength=0.8,\n",
    "        do_cfg = True,\n",
    "        cfg_scale=7.5,\n",
    "        sampler_name=\"ddpm\",\n",
    "        n_inference_steps=50,\n",
    "        models={},\n",
    "        seed=None,\n",
    "        device=None,\n",
    "        idle_device=None,\n",
    "        tokenizer=None,\n",
    "):\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        if not 0 < strength <= 1:\n",
    "            raise ValueError(\"strength must be between 0 and 1\")\n",
    "        \n",
    "\n",
    "        if idle_device:\n",
    "            to_idle = lambda x: x.to(idle_device)\n",
    "\n",
    "        else:\n",
    "            to_idle = lambda x: x \n",
    "\n",
    "\n",
    "        # Initialize random number genrator according to the seed specified \n",
    "        generator = torch.Generator(device=device)\n",
    "\n",
    "        if seed is None:\n",
    "            generator.seed()\n",
    "\n",
    "        else:\n",
    "            generator.manual_seed(seed)\n",
    "\n",
    "\n",
    "        clip = models[\"clip\"]\n",
    "        clip.to(device)\n",
    "\n",
    "\n",
    "        if do_cfg:\n",
    "\n",
    "            # Convert into a list of length seq_len=77\n",
    "            cond_tokens = tokenizer.batch_encode_plus(\n",
    "                [prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids \n",
    "\n",
    "\n",
    "            # (Batch_size, Seq_len)\n",
    "            cond_tokens = torch.tensor(cond_tokens, dtype=torch.long, device=device)\n",
    "\n",
    "            # (Batch_size, seq_len) -> (Batch_size, Seq_len, dim)\n",
    "            cond_context = clip(cond_tokens)\n",
    "\n",
    "            # convert into a list of length seq_len=77 \n",
    "            uncond_tokens = tokenizer.batch_encode_plus(\n",
    "                [uncond_prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids \n",
    "\n",
    "            # (Batch_size, seq_len)\n",
    "            uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=device)\n",
    "\n",
    "            # (Batch_size, seq_len) -> (Batch_size, seq_len, dim)\n",
    "            uncond_context = clip(uncond_tokens)\n",
    "\n",
    "            # (Batch_size, seq_len, Dim) + (Batch_size, seq_len, dim) -> (2 * Batch_size, seq_len, dim)\n",
    "            context = torch.cat([cond_context, uncond_context])\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            # convert into a list of length seq_len=77 \n",
    "            tokens = tokenizer.batch_encode_plus(\n",
    "                [prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids \n",
    "\n",
    "            # (Batch_size, seq_len)\n",
    "            tokens = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "\n",
    "            # (Batch_size, seq_len) -> (Batch_size, seq_len, dim)\n",
    "            context = clip(tokens)\n",
    "\n",
    "        to_idle(clip)\n",
    "\n",
    "\n",
    "\n",
    "        if sampler_name == \"ddpm\":\n",
    "            sampler = DDPMSampler(generator)\n",
    "            sampler.set_inference_timesteps(n_inference_steps)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sampler value %s. \")\n",
    "        \n",
    "\n",
    "        latent_shape = (1, 4, LATENTS_HEIGHT, LATENTS_WIDTH)\n",
    "\n",
    "        if input_image:\n",
    "            encoder = models[\"encoder\"]\n",
    "            encoder.to(device)\n",
    "\n",
    "\n",
    "            input_image_tensor = input_image.resize((WIDTH, HEIGHT))\n",
    "\n",
    "            # (height, width, channels)\n",
    "            input_image_tensor = np.array(input_image_tensor)\n",
    "\n",
    "            # (height, width, channels) -> (height, width, channels)\n",
    "            input_image_tensor = torch.tensor(input_image_tensor, dtype=torch.float32, device=device)\n",
    "\n",
    "            # (height, width, channels) -> (height, width, channels)\n",
    "            input_image_tensor = rescale(input_image_tensor, (0, 255), (-1, 1))\n",
    "\n",
    "            # (height, width, channels) -> (Batch_size, height, width,  channels)\n",
    "            input_image_tensor = input_image_tensor.unsqueeze(0)\n",
    "\n",
    "            # (Batch_size, height, width, channels) -> (Batch_size, height, width, channels)\n",
    "            input_image_tensor = input_image_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "            # (Batch_size, 4, latent_height, latent_width)\n",
    "            encoder_noise = torch.randn(latent_shape, generator=generator, device=device)\n",
    "\n",
    "            # (Batch_size, 4, latent_height, latent_width)\n",
    "            latents = encoder(input_image_tensor, encoder_noise)\n",
    "\n",
    "            # Add noise to the latents (the encoded input image)\n",
    "            # (Batch_size, 4, latent_height, latent_width)\n",
    "            sampler.set_strength(strength=strength)\n",
    "            latents = sampler.add_noise(latents, sampler.timesteps[0])\n",
    "\n",
    "            to_idle(encoder)\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            # (Batch_size, 4, latenent_height, latenent_width)\n",
    "            latents = torch.randn(latent_shape, generator=generator, device=device)\n",
    "\n",
    "        diffusion = models['diffusion']\n",
    "        diffusion.to(device)\n",
    "\n",
    "\n",
    "        timesteps = tqdm(sampler.timesteps)\n",
    "\n",
    "\n",
    "        for i, timesteps in enumerate(timesteps):\n",
    "\n",
    "            # (1, 320)\n",
    "            time_embedding = get_time_embedding(timesteps).to(device)\n",
    "\n",
    "            # (Batch_size, 4, latent_height, latent_width)\n",
    "            model_input = latents\n",
    "\n",
    "            if do_cfg:\n",
    "\n",
    "                # (Batch_size, 4, latetent_height, latent_width) -> (2 * Batch_size, 4, latent_height, latent_width)\n",
    "                model_input = model_input.repeat(2, 1, 1, 1)\n",
    "\n",
    "\n",
    "            # model_output is to predict noise \n",
    "            # (Batch_size, 4, latent_height, latent_width) -> (Batch_size, 4, latent_height, latent_width)\n",
    "            model_output = diffusion(model_input, context, time_embedding)\n",
    "\n",
    "\n",
    "            if do_cfg:\n",
    "                output_cond, output_uncond = model_output.chunk(2)\n",
    "                model_output = cfg_scale * (output_cond - output_uncond) + output_uncond\n",
    "\n",
    "\n",
    "\n",
    "            # (Batch_size, 4, latent_height, latent_width) -> (Batch_size, 4, latent_height, latent_width)\n",
    "            latents = sampler.step(timesteps, latents, model_output)\n",
    "\n",
    "\n",
    "        to_idle(diffusion)\n",
    "\n",
    "        decoder = models[\"decoder\"]\n",
    "        decoder.to(device)\n",
    "\n",
    "        # (Batch_size, 4, latent_height, latent_width) -> (Batch_size, 3, height, width)\n",
    "        images = decoder(latents)\n",
    "\n",
    "        to_idle(decoder)\n",
    "\n",
    "\n",
    "        images = rescale(images, (-1, 1), (0, 255), clamp=True)\n",
    "\n",
    "        # (Batch_size, channels, height, width) -> (Batch_size, height, width, channels)\n",
    "        images = images.permute(0, 2, 3, 1)\n",
    "        images = images.to(\"cpu\", torch.uint8).numpy()\n",
    "        return images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEt's more define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let's break down this block of code line by line:\n",
    "\n",
    "# 0.1\n",
    "\n",
    "### 1. Check if Classifier-Free Guidance (CFG) is Enabled:\n",
    "```python\n",
    "if do_cfg:\n",
    "```\n",
    "- This checks if the `do_cfg` flag is `True`. If it is, the function will perform specific operations to support classifier-free guidance.\n",
    "\n",
    "### 2. Tokenize the Conditional Prompt:\n",
    "```python\n",
    "    # Convert into a list of length seq_len=77\n",
    "    cond_tokens = tokenizer.batch_encode_plus(\n",
    "        [prompt], padding=\"max_length\", max_length=77\n",
    "    ).input_ids\n",
    "```\n",
    "- **Tokenization**: Uses the tokenizer to encode the `prompt` into tokens. \n",
    "- **Padding and Length**: Ensures the token list has a fixed length of 77, padding if necessary.\n",
    "- **Output**: `cond_tokens` contains the encoded tokens for the prompt as a list of integers.\n",
    "\n",
    "### 3. Convert Tokens to Tensor:\n",
    "```python\n",
    "    # (Batch_size, Seq_len)\n",
    "    cond_tokens = torch.tensor(cond_tokens, dtype=torch.long, device=device)\n",
    "```\n",
    "- **Convert to Tensor**: Converts the list of tokens into a PyTorch tensor with data type `long`.\n",
    "- **Device**: Moves the tensor to the specified device (e.g., CPU or GPU).\n",
    "\n",
    "### 4. Get Conditional Context:\n",
    "```python\n",
    "    # (Batch_size, seq_len) -> (Batch_size, Seq_len, dim)\n",
    "    cond_context = clip(cond_tokens)\n",
    "```\n",
    "- **Context Embedding**: Passes the tensor of conditional tokens through the CLIP model to obtain contextual embeddings.\n",
    "- **Shape**: The output shape is `(Batch_size, seq_len, dim)`, where `dim` is the embedding dimension produced by the CLIP model.\n",
    "\n",
    "### 5. Tokenize the Unconditional Prompt:\n",
    "```python\n",
    "    # convert into a list of length seq_len=77 \n",
    "    uncond_tokens = tokenizer.batch_encode_plus(\n",
    "        [uncond_prompt], padding=\"max_length\", max_length=77\n",
    "    ).input_ids\n",
    "```\n",
    "- **Tokenization**: Similar to the conditional prompt, this tokenizes the `uncond_prompt` with a fixed length of 77, padding if necessary.\n",
    "- **Output**: `uncond_tokens` contains the encoded tokens for the unconditional prompt as a list of integers.\n",
    "\n",
    "### 6. Convert Unconditional Tokens to Tensor:\n",
    "```python\n",
    "    # (Batch_size, seq_len)\n",
    "    uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=device)\n",
    "```\n",
    "- **Convert to Tensor**: Converts the list of unconditional tokens into a PyTorch tensor with data type `long`.\n",
    "- **Device**: Moves the tensor to the specified device (e.g., CPU or GPU).\n",
    "\n",
    "### 7. Get Unconditional Context:\n",
    "```python\n",
    "    # (Batch_size, seq_len) -> (Batch_size, seq_len, dim)\n",
    "    uncond_context = clip(uncond_tokens)\n",
    "```\n",
    "- **Context Embedding**: Passes the tensor of unconditional tokens through the CLIP model to obtain contextual embeddings.\n",
    "- **Shape**: The output shape is `(Batch_size, seq_len, dim)`, where `dim` is the embedding dimension produced by the CLIP model.\n",
    "\n",
    "### 8. Concatenate Contexts:\n",
    "```python\n",
    "    # (Batch_size, seq_len, Dim) + (Batch_size, seq_len, dim) -> (2 * Batch_size, seq_len, dim)\n",
    "    context = torch.cat([cond_context, uncond_context])\n",
    "```\n",
    "- **Concatenation**: Combines the conditional and unconditional context embeddings along the batch dimension.\n",
    "- **Shape**: The resulting `context` tensor has the shape `(2 * Batch_size, seq_len, dim)`, effectively doubling the batch size.\n",
    "\n",
    "### Summary\n",
    "This block of code is responsible for preparing the contextual embeddings needed for classifier-free guidance. It tokenizes the text prompts, converts them into tensors, obtains context embeddings using the CLIP model, and concatenates the embeddings to form a final `context` tensor.\n",
    "\n",
    "If you have more questions or need further clarification on any part, feel free to ask! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "if do_cfg:\n",
    "\n",
    "            # Convert into a list of length seq_len=77\n",
    "            cond_tokens = tokenizer.batch_encode_plus(\n",
    "                [prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids \n",
    "\n",
    "\n",
    "            # (Batch_size, Seq_len)\n",
    "            cond_tokens = torch.tensor(cond_tokens, dtype=torch.long, device=device)\n",
    "\n",
    "            # (Batch_size, seq_len) -> (Batch_size, Seq_len, dim)\n",
    "            cond_context = clip(cond_tokens)\n",
    "\n",
    "            # convert into a list of length seq_len=77 \n",
    "            uncond_tokens = tokenizer.batch_encode_plus(\n",
    "                [uncond_prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids \n",
    "\n",
    "            # (Batch_size, seq_len)\n",
    "            uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=device)\n",
    "\n",
    "            # (Batch_size, seq_len) -> (Batch_size, seq_len, dim)\n",
    "            uncond_context = clip(uncond_tokens)\n",
    "\n",
    "            # (Batch_size, seq_len, Dim) + (Batch_size, seq_len, dim) -> (2 * Batch_size, seq_len, dim)\n",
    "            context = torch.cat([cond_context, uncond_context])\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "            # convert into a list of length seq_len=77 \n",
    "            tokens = tokenizer.batch_encode_plus(\n",
    "                [prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids \n",
    "\n",
    "            # (Batch_size, seq_len)\n",
    "            tokens = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "\n",
    "            # (Batch_size, seq_len) -> (Batch_size, seq_len, dim)\n",
    "            context = clip(tokens)\n",
    "\n",
    "to_idle(clip)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's go through this block of code line by line:\n",
    "\n",
    "# 0.2\n",
    "\n",
    "### **1. Define Latent Shape:**\n",
    "```python\n",
    "latent_shape = (1, 4, LATENTS_HEIGHT, LATENTS_WIDTH)\n",
    "```\n",
    "- **Latent Dimensions**: Sets the shape of the latents, with dimensions `(Batch_size=1, Channels=4, Height=LATENTS_HEIGHT, Width=LATENTS_WIDTH)`.\n",
    "\n",
    "### **2. Check if Input Image is Provided:**\n",
    "```python\n",
    "if input_image:\n",
    "```\n",
    "- **Condition**: Checks if an `input_image` is provided. If it is, the subsequent block of code will execute.\n",
    "\n",
    "### **3. Load and Prepare Encoder Model:**\n",
    "```python\n",
    "    encoder = models[\"encoder\"]\n",
    "    encoder.to(device)\n",
    "```\n",
    "- **Load Encoder**: Loads the encoder model from the `models` dictionary.\n",
    "- **Move to Device**: Moves the encoder model to the specified `device` (e.g., CPU or GPU).\n",
    "\n",
    "### **4. Resize Input Image:**\n",
    "```python\n",
    "    input_image_tensor = input_image.resize((WIDTH, HEIGHT))\n",
    "```\n",
    "- **Resize**: Resizes the input image to match the `WIDTH` and `HEIGHT` defined earlier (512x512 pixels).\n",
    "\n",
    "### **5. Convert Image to NumPy Array:**\n",
    "```python\n",
    "    # (height, width, channels)\n",
    "    input_image_tensor = np.array(input_image_tensor)\n",
    "```\n",
    "- **NumPy Conversion**: Converts the resized input image into a NumPy array with shape `(Height, Width, Channels)`.\n",
    "\n",
    "### **6. Convert Image to PyTorch Tensor:**\n",
    "```python\n",
    "    # (height, width, channels) -> (height, width, channels)\n",
    "    input_image_tensor = torch.tensor(input_image_tensor, dtype=torch.float32, device=device)\n",
    "```\n",
    "- **Tensor Conversion**: Converts the NumPy array into a PyTorch tensor with data type `float32` and moves it to the specified `device`.\n",
    "\n",
    "### **7. Rescale Image Values:**\n",
    "```python\n",
    "    # (height, width, channels) -> (height, width, channels)\n",
    "    input_image_tensor = rescale(input_image_tensor, (0, 255), (-1, 1))\n",
    "```\n",
    "- **Rescaling**: Rescales the image values from the range `(0, 255)` to `(-1, 1)` using the `rescale` function.\n",
    "\n",
    "### **8. Add Batch Dimension:**\n",
    "```python\n",
    "    # (height, width, channels) -> (Batch_size, height, width,  channels)\n",
    "    input_image_tensor = input_image_tensor.unsqueeze(0)\n",
    "```\n",
    "- **Unsqueeze**: Adds a batch dimension to the tensor, changing its shape to `(Batch_size, Height, Width, Channels)`.\n",
    "\n",
    "### **9. Permute Tensor Dimensions:**\n",
    "```python\n",
    "    # (Batch_size, height, width, channels) -> (Batch_size, height, width, channels)\n",
    "    input_image_tensor = input_image_tensor.permute(0, 3, 1, 2)\n",
    "```\n",
    "- **Permute**: Rearranges the dimensions of the tensor to match the shape `(Batch_size, Channels, Height, Width)`.\n",
    "\n",
    "### **10. Generate Encoder Noise:**\n",
    "```python\n",
    "    # (Batch_size, 4, latent_height, latent_width)\n",
    "    encoder_noise = torch.randn(latent_shape, generator=generator, device=device)\n",
    "```\n",
    "- **Random Noise**: Generates a tensor of random noise with the shape `(Batch_size, 4, LATENTS_HEIGHT, LATENTS_WIDTH)` using the specified random `generator`.\n",
    "\n",
    "### **11. Encode Input Image:**\n",
    "```python\n",
    "    # (Batch_size, 4, latent_height, latent_width)\n",
    "    latents = encoder(input_image_tensor, encoder_noise)\n",
    "```\n",
    "- **Encoding**: Passes the input image tensor and the random noise through the encoder to generate latents.\n",
    "\n",
    "### **12. Add Noise to Latents:**\n",
    "```python\n",
    "    # Add noise to the latents (the encoded input image)\n",
    "    # (Batch_size, 4, latent_height, latent_width)\n",
    "    sampler.set_strength(strength=strength)\n",
    "    latents = sampler.add_noise(latents, sampler.timesteps[0])\n",
    "```\n",
    "- **Set Strength**: Configures the `strength` parameter for the sampler.\n",
    "- **Add Noise**: Adds noise to the latents based on the configured strength and the initial timestep of the sampler.\n",
    "\n",
    "### **13. Move Encoder to Idle Device:**\n",
    "```python\n",
    "    to_idle(encoder)\n",
    "```\n",
    "- **Device Handling**: Moves the encoder model to the `idle_device` (if specified) to free up resources on the main `device`.\n",
    "\n",
    "This breakdown provides a detailed explanation of each step in the code, focusing on how the input image is processed, encoded, and prepared for the subsequent stages of the generation process. If you have any further questions or need more details on any part, feel free to ask! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "        latents_shape = (1, 4, LATENTS_HEIGHT, LATENTS_WIDTH)\n",
    "\n",
    "        if input_image:\n",
    "            encoder = models[\"encoder\"]\n",
    "            encoder.to(device)\n",
    "\n",
    "            input_image_tensor = input_image.resize((WIDTH, HEIGHT))\n",
    "            # (Height, Width, Channel)\n",
    "            input_image_tensor = np.array(input_image_tensor)\n",
    "            # (Height, Width, Channel) -> (Height, Width, Channel)\n",
    "            input_image_tensor = torch.tensor(input_image_tensor, dtype=torch.float32, device=device)\n",
    "            # (Height, Width, Channel) -> (Height, Width, Channel)\n",
    "            input_image_tensor = rescale(input_image_tensor, (0, 255), (-1, 1))\n",
    "            # (Height, Width, Channel) -> (Batch_Size, Height, Width, Channel)\n",
    "            input_image_tensor = input_image_tensor.unsqueeze(0)\n",
    "            # (Batch_Size, Height, Width, Channel) -> (Batch_Size, Channel, Height, Width)\n",
    "            input_image_tensor = input_image_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            encoder_noise = torch.randn(latents_shape, generator=generator, device=device)\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            latents = encoder(input_image_tensor, encoder_noise)\n",
    "\n",
    "            # Add noise to the latents (the encoded input image)\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            sampler.set_strength(strength=strength)\n",
    "            latents = sampler.add_noise(latents, sampler.timesteps[0])\n",
    "\n",
    "            to_idle(encoder)\n",
    "        else:\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            latents = torch.randn(latents_shape, generator=generator, device=device)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.3\n",
    "Let's break down this block of code line by line:\n",
    "\n",
    "### **1. Initialize Timesteps:**\n",
    "```python\n",
    "timesteps = tqdm(sampler.timesteps)\n",
    "```\n",
    "- **Timesteps with Progress Bar**: Wraps the sampler's timesteps with `tqdm` to show a progress bar. `sampler.timesteps` is a list of timesteps used in the diffusion process.\n",
    "\n",
    "### **2. Loop Through Timesteps:**\n",
    "```python\n",
    "for i, timestep in enumerate(timesteps):\n",
    "```\n",
    "- **Iterate**: Loops through each `timestep` in the timesteps list. `i` is the index of the current timestep.\n",
    "\n",
    "### **3. Generate Time Embedding:**\n",
    "```python\n",
    "    # (1, 320)\n",
    "    time_embedding = get_time_embedding(timestep).to(device)\n",
    "```\n",
    "- **Get Embedding**: Generates a time embedding for the current timestep using the `get_time_embedding` function.\n",
    "- **Move to Device**: Moves the embedding to the specified `device` (e.g., CPU or GPU).\n",
    "\n",
    "### **4. Prepare Model Input:**\n",
    "```python\n",
    "    # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "    model_input = latents\n",
    "```\n",
    "- **Set Input**: Assigns the `latents` tensor as the initial model input.\n",
    "\n",
    "### **5. Handle Classifier-Free Guidance (CFG):**\n",
    "```python\n",
    "    if do_cfg:\n",
    "        # (Batch_Size, 4, Latents_Height, Latents_Width) -> (2 * Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "        model_input = model_input.repeat(2, 1, 1, 1)\n",
    "```\n",
    "- **Repeat Latents**: If `do_cfg` is `True`, duplicates the `model_input` along the batch dimension, effectively doubling the batch size. This prepares the input for classifier-free guidance.\n",
    "\n",
    "### **6. Diffusion Model Forward Pass:**\n",
    "```python\n",
    "    # model_output is the predicted noise\n",
    "    # (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "    model_output = diffusion(model_input, context, time_embedding)\n",
    "```\n",
    "- **Predict Noise**: Passes the `model_input`, `context`, and `time_embedding` through the diffusion model to predict the noise. The output is a tensor of the same shape as the input.\n",
    "\n",
    "### **7. Apply Classifier-Free Guidance (CFG):**\n",
    "```python\n",
    "    if do_cfg:\n",
    "        output_cond, output_uncond = model_output.chunk(2)\n",
    "        model_output = cfg_scale * (output_cond - output_uncond) + output_uncond\n",
    "```\n",
    "- **Chunk Output**: Splits the `model_output` tensor into two parts: `output_cond` (conditional) and `output_uncond` (unconditional).\n",
    "- **Combine Outputs**: Combines the two outputs using the guidance scale `cfg_scale`. The formula used here enhances the conditional output while retaining some influence from the unconditional output.\n",
    "\n",
    "### **8. Update Latents:**\n",
    "```python\n",
    "    # (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "    latents = sampler.step(timestep, latents, model_output)\n",
    "```\n",
    "- **Denoise Step**: Uses the sampler to update the `latents` based on the predicted noise (`model_output`) and the current `timestep`. This step iteratively refines the latents throughout the diffusion process.\n",
    "\n",
    "### **Summary:**\n",
    "This block of code performs the core iterative process of the diffusion model:\n",
    "1. **Time Embedding**: Generates time embeddings for each timestep.\n",
    "2. **Model Input**: Prepares the model input, accounting for classifier-free guidance if enabled.\n",
    "3. **Predict Noise**: Uses the diffusion model to predict the noise.\n",
    "4. **CFG Application**: Applies classifier-free guidance to refine the prediction.\n",
    "5. **Latents Update**: Updates the latents using the sampler.\n",
    "\n",
    "This iterative process helps refine the latents, gradually denoising them to produce the final image. If you have more questions or need further clarification, feel free to ask! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "        timesteps = tqdm(sampler.timesteps)\n",
    "        for i, timestep in enumerate(timesteps):\n",
    "            # (1, 320)\n",
    "            time_embedding = get_time_embedding(timestep).to(device)\n",
    "\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            model_input = latents\n",
    "\n",
    "            if do_cfg:\n",
    "                # (Batch_Size, 4, Latents_Height, Latents_Width) -> (2 * Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "                model_input = model_input.repeat(2, 1, 1, 1)\n",
    "\n",
    "            # model_output is the predicted noise\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            model_output = diffusion(model_input, context, time_embedding)\n",
    "\n",
    "            if do_cfg:\n",
    "                output_cond, output_uncond = model_output.chunk(2)\n",
    "                model_output = cfg_scale * (output_cond - output_uncond) + output_uncond\n",
    "\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            latents = sampler.step(timestep, latents, model_output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.4 \n",
    "\n",
    "Sure, let's break down this block of code line by line:\n",
    "\n",
    "### **1. Load and Prepare Decoder Model:**\n",
    "```python\n",
    "decoder = models[\"decoder\"]\n",
    "decoder.to(device)\n",
    "```\n",
    "- **Load Decoder**: Loads the decoder model from the `models` dictionary.\n",
    "- **Move to Device**: Moves the decoder model to the specified `device` (e.g., CPU or GPU).\n",
    "\n",
    "### **2. Decode Latents:**\n",
    "```python\n",
    "# (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 3, Height, Width)\n",
    "images = decoder(latents)\n",
    "```\n",
    "- **Decode Latents**: Passes the `latents` tensor through the decoder model to generate images. The output tensor shape changes from `(Batch_Size, 4, Latents_Height, Latents_Width)` to `(Batch_Size, 3, Height, Width)`, where `3` represents the RGB color channels.\n",
    "\n",
    "### **3. Move Decoder to Idle Device:**\n",
    "```python\n",
    "to_idle(decoder)\n",
    "```\n",
    "- **Device Handling**: Moves the decoder model to the `idle_device` (if specified) to free up resources on the main `device`.\n",
    "\n",
    "### **4. Rescale Image Values:**\n",
    "```python\n",
    "images = rescale(images, (-1, 1), (0, 255), clamp=True)\n",
    "```\n",
    "- **Rescaling**: Rescales the image tensor values from the range `(-1, 1)` to `(0, 255)` using the `rescale` function, ensuring the values are clamped within the specified range. This prepares the image for display or saving in a standard image format.\n",
    "\n",
    "### **5. Permute Tensor Dimensions:**\n",
    "```python\n",
    "# (Batch_Size, Channel, Height, Width) -> (Batch_Size, Height, Width, Channel)\n",
    "images = images.permute(0, 2, 3, 1)\n",
    "```\n",
    "- **Permute**: Rearranges the dimensions of the tensor from `(Batch_Size, Channels, Height, Width)` to `(Batch_Size, Height, Width, Channels)` to match the standard image format.\n",
    "\n",
    "### **6. Convert Tensor to NumPy Array:**\n",
    "```python\n",
    "images = images.to(\"cpu\", torch.uint8).numpy()\n",
    "```\n",
    "- **Move to CPU**: Moves the tensor to the CPU.\n",
    "- **Convert to Unsigned 8-bit Integer**: Converts the tensor data type to `uint8` for image processing.\n",
    "- **Convert to NumPy Array**: Converts the tensor to a NumPy array.\n",
    "\n",
    "### **7. Return First Image:**\n",
    "```python\n",
    "return images[0]\n",
    "```\n",
    "- **Return**: Returns the first image from the batch.\n",
    "\n",
    "### **Summary:**\n",
    "This block of code takes the refined latents and decodes them into images. The steps include loading the decoder model, generating images from the latents, rescaling and permuting the image tensor, converting it to a NumPy array, and returning the final image.\n",
    "\n",
    "If you have any further questions or need more details, feel free to ask! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "        to_idle(diffusion)\n",
    "\n",
    "        decoder = models[\"decoder\"]\n",
    "        decoder.to(device)\n",
    "        # (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 3, Height, Width)\n",
    "        images = decoder(latents)\n",
    "        to_idle(decoder)\n",
    "\n",
    "        images = rescale(images, (-1, 1), (0, 255), clamp=True)\n",
    "        # (Batch_Size, Channel, Height, Width) -> (Batch_Size, Height, Width, Channel)\n",
    "        images = images.permute(0, 2, 3, 1)\n",
    "        images = images.to(\"cpu\", torch.uint8).numpy()\n",
    "        return images[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyramid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
